# -*- coding: utf-8 -*-  

"""
Created on 2024/8/20

@author: Ruoyu Chen
"""

import argparse

import os
import cv2
import numpy as np
import tensorflow as tf
from PIL import Image
from matplotlib import pyplot as plt

from xplique.wrappers import TorchWrapper
from xplique.metrics import MuFidelity, Insertion, Deletion

import timm

import torch
import torchvision.transforms as transforms
import torchvision.models as models

from transformers import AutoModelForImageClassification
from PIL import Image
from timm.data.transforms_factory import create_transform

from tqdm import tqdm

from vim.models_mamba import vim_base_patch16_224_bimambav2_final_pool_mean_abs_pos_embed_with_middle_cls_token_div2
from timm.models import create_model

tf.config.run_functions_eagerly(True)

gpus = tf.config.experimental.list_physical_devices(device_type='GPU')
tf.config.experimental.set_virtual_device_configuration(
    gpus[0],
    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4048)]
)

def read_audio(
    audio_path,
    device,
    num_mel_bins=128,
    target_length=204,
    sample_rate=16000,
    mean= -4.268, 
    std= 9.138
):
    waveform, sr = torchaudio.load(audio_path)
    if sample_rate != sr:
        waveform = torchaudio.functional.resample(
            waveform, orig_freq=sr, new_freq=sample_rate
        )
    all_clips_timepoints = get_clip_timepoints(
        clip_sampler, waveform.size(1) / sample_rate
    )
    all_clips = []
    for clip_timepoints in all_clips_timepoints:
        waveform_clip = waveform[
            :,
            int(clip_timepoints[0] * sample_rate) : int(
                clip_timepoints[1] * sample_rate
            ),
        ]
        waveform_melspec = waveform2melspec(
            waveform_clip, sample_rate, num_mel_bins, target_length
        )
        all_clips.append(waveform_melspec)

    normalize = transforms.Normalize(mean=mean, std=std)
    all_clips = [normalize(ac).to(device) for ac in all_clips]

    all_clips = torch.stack(all_clips, dim=0)
    return all_clips.cpu().numpy()

def parse_args():
    parser = argparse.ArgumentParser(description='Deletion Metric')
    # general
    parser.add_argument('--Datasets',
                        type=str,
                        default='datasets/imagenet/ILSVRC2012_img_val',
                        help='Datasets.')
    parser.add_argument('--eval-list',
                        type=str,
                        default='datasets/imagenet/val_vim_5k_true.txt',
                        help='Datasets.')
    parser.add_argument('--eval-number',
                        type=int,
                        default=None,
                        help='Datasets.')
    parser.add_argument('--explanation-method', 
                        type=str, 
                        default='./explanation_results/imagenet-vim-true/KernelShap',
                        help='Save path for saliency maps generated by interpretability methods.')
    args = parser.parse_args()
    return args
    
def main(args):
    class_number = 1000
    
    # data preproccess
    with open(args.eval_list, "r") as f:
        datas = f.read().split('\n')
        
    label = []
    input_image = []
    explanations = []
    
    for data in tqdm(datas[ : args.eval_number]):
        label.append(int(data.strip().split(" ")[-1]))
        input_image.append(
            transform_vision_data(os.path.join(args.Datasets, data.split(" ")[0]))
        )
        explanations.append(
            np.load(
                os.path.join(args.explanation_method, data.split(" ")[0].replace(".JPEG", ".npy")))
        )
        
    label_onehot = tf.one_hot(np.array(label), class_number)
    input_image = np.array(input_image)
    explanations = np.array(explanations)
    
    device = "cuda:1" if torch.cuda.is_available() else "cpu"
    vis_model = create_model(
            "vim_base_patch16_224_bimambav2_final_pool_mean_abs_pos_embed_with_middle_cls_token_div2",
            pretrained=False,
            num_classes=1000,
            drop_rate=0.01,
            drop_path_rate=0.05,
            drop_block_rate=None,
            img_size=224
        )
    checkpoint = torch.load("ckpt/pytorch_model/vim_b_midclstok_81p9acc.pth", map_location='cpu')

    checkpoint_model = checkpoint['model']
    
    state_dict = vis_model.state_dict()
    
    # interpolate position embedding
    pos_embed_checkpoint = checkpoint_model['pos_embed']
    embedding_size = pos_embed_checkpoint.shape[-1]
    num_patches = vis_model.patch_embed.num_patches
    num_extra_tokens = vis_model.pos_embed.shape[-2] - num_patches
    # height (== width) for the checkpoint position embedding
    orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)
    # height (== width) for the new position embedding
    new_size = int(num_patches ** 0.5)
    # class_token and dist_token are kept unchanged
    extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]
    # only the position tokens are interpolated
    pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]
    pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)
    pos_tokens = torch.nn.functional.interpolate(
    pos_tokens, size=(new_size, new_size), mode='bicubic', align_corners=False)
    pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)
    new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)
    checkpoint_model['pos_embed'] = new_pos_embed

    vis_model.load_state_dict(checkpoint_model, strict=False)
    
    vis_model.eval()
    vis_model.to(device)
    print("load Vision Mamba model")
    
    model = TorchWrapper(vis_model.eval(), device)
    
    # original
    deletion_metric = Deletion(model, input_image, label_onehot, steps=50, activation="softmax", batch_size=32)
    insertion_metric = Insertion(model, input_image, label_onehot, steps=50, activation="softmax", batch_size=32)

    deletion_score_org = deletion_metric(explanations)
    insertion_score_org = insertion_metric(explanations)
    
    print("{} Attribution Method Deletion Score: {}".format(args.explanation_method.split("/")[-1], deletion_score_org))
    print("{} Attribution Method Insertion Score: {}".format(args.explanation_method.split("/")[-1], insertion_score_org))
    return 
    
if __name__ == "__main__":
    args = parse_args()
    main(args)