# -*- coding: utf-8 -*-  

"""
Created on 2024/8/20

@author: Ruoyu Chen
"""

import argparse

import os
import cv2
import numpy as np
import tensorflow as tf
from PIL import Image
from matplotlib import pyplot as plt

from xplique.wrappers import TorchWrapper
from xplique.metrics import MuFidelity, Insertion, Deletion

import timm

import torch
import torchvision.transforms as transforms
import torchvision.models as models

from transformers import AutoModelForImageClassification
from PIL import Image
from timm.data.transforms_factory import create_transform

from tqdm import tqdm

tf.config.run_functions_eagerly(True)

gpus = tf.config.experimental.list_physical_devices(device_type='GPU')
tf.config.experimental.set_virtual_device_configuration(
    gpus[0],
    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4048)]
)

data_transform = create_transform(input_size=(3, 224, 224),
                             is_training=False,
                             mean=[0.485, 0.456, 0.406],
                             std=[0.229, 0.224, 0.225])

def transform_vision_data(image_path, channel_first=False):
    """
    Input:
        image: An image read by opencv [w,h,c]
    Output:
        image: After preproccessing, is a tensor [c,w,h]
    """
    image =cv2.imread(image_path)
    
    image = Image.fromarray(image)
    image = data_transform(image)
    if channel_first:
        pass
    else:
        image = image.permute(1,2,0)
    return image.numpy()

class MambaVision_Super(torch.nn.Module):
    def __init__(self, 
                 model,
                 device = "cuda"):
        super().__init__()
        self.model = model
        self.device = device
    
    def forward(self, vision_inputs):
        with torch.no_grad():
            predicted_scores = self.model(vision_inputs)['logits']
        return predicted_scores

def parse_args():
    parser = argparse.ArgumentParser(description='Deletion Metric')
    # general
    parser.add_argument('--Datasets',
                        type=str,
                        default='datasets/imagenet/ILSVRC2012_img_val',
                        help='Datasets.')
    parser.add_argument('--eval-list',
                        type=str,
                        default='datasets/imagenet/val_mambavision_5k_true.txt',
                        help='Datasets.')
    parser.add_argument('--eval-number',
                        type=int,
                        default=None,
                        help='Datasets.')
    parser.add_argument('--explanation-method', 
                        type=str, 
                        default='./explanation_results/imagenet-mambavision-true/KernelShap',
                        help='Save path for saliency maps generated by interpretability methods.')
    args = parser.parse_args()
    return args
    
def main(args):
    class_number = 1000
    
    # data preproccess
    with open(args.eval_list, "r") as f:
        datas = f.read().split('\n')
        
    label = []
    input_image = []
    explanations = []
    
    for data in tqdm(datas[ : args.eval_number]):
        label.append(int(data.strip().split(" ")[-1]))
        input_image.append(
            transform_vision_data(os.path.join(args.Datasets, data.split(" ")[0]))
        )
        explanations.append(
            np.load(
                os.path.join(args.explanation_method, data.split(" ")[0].replace(".JPEG", ".npy")))
        )
        
    label_onehot = tf.one_hot(np.array(label), class_number)
    input_image = np.array(input_image)
    explanations = np.array(explanations)
    
    device = "cuda:1" if torch.cuda.is_available() else "cpu"
    vis_model = AutoModelForImageClassification.from_pretrained("nvidia/MambaVision-L2-1K", trust_remote_code=True)
    vis_model = MambaVision_Super(vis_model, device)
    vis_model.eval()
    vis_model.to(device)
    print("load MambaVision model")
    
    model = TorchWrapper(vis_model.eval(), device)
    
    # original
    deletion_metric = Deletion(model, input_image, label_onehot, steps=50, activation="softmax")
    insertion_metric = Insertion(model, input_image, label_onehot, steps=50, activation="softmax")

    deletion_score_org = deletion_metric(explanations)
    insertion_score_org = insertion_metric(explanations)
    
    print("{} Attribution Method Deletion Score: {}".format(args.explanation_method.split("/")[-1], deletion_score_org))
    print("{} Attribution Method Insertion Score: {}".format(args.explanation_method.split("/")[-1], insertion_score_org))
    return 
    
if __name__ == "__main__":
    args = parse_args()
    main(args)