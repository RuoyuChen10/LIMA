# -*- coding: utf-8 -*- 


"""
Created on 2024/8/23

@author: Ruoyu Chen
"""

import argparse

import os
import cv2
import numpy as np
import tensorflow as tf
from PIL import Image
from matplotlib import pyplot as plt

from xplique.wrappers import TorchWrapper
from xplique.metrics import MuFidelity, Insertion, Deletion

from imagebind import data
from imagebind.models import imagebind_model
from imagebind.models.imagebind_model import ModalityType
from imagebind.data import waveform2melspec, get_clip_timepoints

import torch
from torchvision import transforms
from utils import *

from tqdm import tqdm

import torchaudio
from pytorchvideo.data.clip_sampling import ConstantClipsPerVideoSampler

clip_sampler = ConstantClipsPerVideoSampler(
    clip_duration=2, clips_per_video=3
)

def parse_args():
    parser = argparse.ArgumentParser(description='Deletion Metric')
    # general
    parser.add_argument('--Datasets',
                        type=str,
                        default='datasets/vggsound/test',
                        help='Datasets.')
    parser.add_argument('--eval-list',
                        type=str,
                        default='datasets/vggsound/val_imagebind_600_true.txt',
                        help='Datasets.')
    parser.add_argument('--eval-number',
                        type=int,
                        default=-1,
                        help='Datasets.')
    parser.add_argument('--explanation-method', 
                        type=str, 
                        default='./explanation_results/vggsound-imagebind-true/KernelShap',
                        help='Save path for saliency maps generated by interpretability methods.')
    args = parser.parse_args()
    return args

class ImageBindModel_Super(torch.nn.Module):
    def __init__(self, base_model):
        super().__init__()
        self.base_model = base_model
        self.semantic_modal = None
        
    def forward(self, audio_inputs):
        """
        Input:
            audio_inputs: torch.size([B,C,W,H])
        Output:
            embeddings: a d-dimensional vector torch.size([B,d])
        """
        audio_inputs = audio_inputs.unsqueeze(2)
        
        inputs = {
            "audio": audio_inputs,
        }
        
        # with torch.no_grad():
        embeddings = self.base_model(inputs)
        
        scores = torch.softmax(embeddings["audio"] @ self.semantic_modal.T, dim=-1)
        return scores
    
def read_audio(
    audio_path,
    device="cpu",
    num_mel_bins=128,
    target_length=204,
    sample_rate=16000,
    mean= -4.268, 
    std= 9.138
):
    waveform, sr = torchaudio.load(audio_path)
    if sample_rate != sr:
        waveform = torchaudio.functional.resample(
            waveform, orig_freq=sr, new_freq=sample_rate
        )
    all_clips_timepoints = get_clip_timepoints(
        clip_sampler, waveform.size(1) / sample_rate
    )
    all_clips = []
    for clip_timepoints in all_clips_timepoints:
        waveform_clip = waveform[
            :,
            int(clip_timepoints[0] * sample_rate) : int(
                clip_timepoints[1] * sample_rate
            ),
        ]
        waveform_melspec = waveform2melspec(
            waveform_clip, sample_rate, num_mel_bins, target_length
        )
        all_clips.append(waveform_melspec)

    normalize = transforms.Normalize(mean=mean, std=std)
    all_clips = [normalize(ac).to(device) for ac in all_clips]

    all_clips = torch.stack(all_clips, dim=0)
    
    out = all_clips[:,0,:,:].cpu().numpy()
    out = out.transpose(1,2,0)
    return out

def load_and_transform_audio_data(audio_paths):
    data = []
    for audio_path in audio_paths:
        data.append(read_audio(audio_path))
        
    return np.array(data)

def main(args):
    class_number = 309
    
    # data preproccess
    with open(args.eval_list, "r") as f:
        datas = f.read().split('\n')

    label = []
    input_audio = []
    explanations = []

    for data in tqdm(datas[ : args.eval_number]):
        label.append(int(data.strip().split(" ")[-1]))
        input_audio.append(
           read_audio(os.path.join(args.Datasets, data.split(" ")[0]))
        )
        explanations.append(
            np.load(
                os.path.join(args.explanation_method, data.split(" ")[0].replace(".flac", ".npy")))
        )
    
    label_onehot = tf.one_hot(np.array(label), class_number)
    input_audio = np.array(input_audio)
    explanations = np.array(explanations)
    
    device = "cuda:1" if torch.cuda.is_available() else "cpu"
    # Load model
    model = imagebind_model.imagebind_huge(pretrained=True)
    model.eval()
    model.to(device)
    
    audio_model = ImageBindModel_Super(model)
    print("load imagebind model")
    
    semantic_path = "ckpt/semantic_features/vggsound_imagebind_cls.pt"
    if os.path.exists(semantic_path):
        semantic_feature = torch.load(semantic_path, map_location="cpu")
        semantic_feature = semantic_feature.to(device) * 0.05

    audio_model.semantic_modal = semantic_feature
    
    model = TorchWrapper(audio_model.eval(), device)
    
    # original
    deletion_metric = Deletion(model, input_audio, label_onehot, steps=50, batch_size=32)
    insertion_metric = Insertion(model, input_audio, label_onehot, steps=50, batch_size=32)
    
    deletion_score_org = deletion_metric(explanations)
    insertion_score_org = insertion_metric(explanations)
    
    print("{} Attribution Method Deletion Score: {}".format(args.explanation_method.split("/")[-1], deletion_score_org))
    print("{} Attribution Method Insertion Score: {}".format(args.explanation_method.split("/")[-1], insertion_score_org))
    return 

if __name__ == "__main__":
    args = parse_args()
    main(args)