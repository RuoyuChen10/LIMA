# -*- coding: utf-8 -*- 


"""
Created on 2024/8/23

@author: Ruoyu Chen
"""

import argparse

import os
import cv2
import numpy as np
import tensorflow as tf
from PIL import Image
from matplotlib import pyplot as plt

from xplique.wrappers import TorchWrapper
from xplique.metrics import MuFidelity, Insertion, Deletion

from languagebind import LanguageBind, to_device, transform_dict, LanguageBindImageTokenizer

import torch
from torchvision import transforms
from utils import *

from tqdm import tqdm


def parse_args():
    parser = argparse.ArgumentParser(description='Deletion Metric')
    # general
    parser.add_argument('--Datasets',
                        type=str,
                        default='datasets/vggsound/test',
                        help='Datasets.')
    parser.add_argument('--eval-list',
                        type=str,
                        default='datasets/vggsound/val_languagebind_600_true.txt',
                        help='Datasets.')
    parser.add_argument('--eval-number',
                        type=int,
                        default=-1,
                        help='Datasets.')
    parser.add_argument('--explanation-method', 
                        type=str, 
                        default='./explanation_results/vggsound-languagebind-true/KernelShap',
                        help='Save path for saliency maps generated by interpretability methods.')
    args = parser.parse_args()
    return args

class LanguageBindModel_Super(torch.nn.Module):
    def __init__(self, base_model):
        super().__init__()
        self.base_model = base_model
        self.semantic_modal = None
        
    def forward(self, audio_inputs):
        """
        Input:
            audio_inputs: torch.size([B,C,W,H]) # video
        Output:
            embeddings: a d-dimensional vector torch.size([B,d])
        """
        inputs = {
            "audio": {'pixel_values': audio_inputs},
        }
        
        with torch.no_grad():
            embeddings = self.base_model(inputs)
            
        scores = torch.softmax(embeddings["audio"] @ self.semantic_modal.T, dim=-1)
        
        return scores
    
def read_audio(
    audio_path,
    modality_transform,
    device = "cpu"
):
    audio = [audio_path]
    audio_proccess = to_device(modality_transform['audio'](audio), device)['pixel_values'][0]
    return audio_proccess.cpu().numpy().transpose(1,2,0) # [w,h,c]

def load_and_transform_audio_data(audio_paths, modality_transform):
    data = []
    for audio_path in audio_paths:
        data.append(read_audio(audio_path, modality_transform))
        
    return np.array(data)

def main(args):
    class_number = 309
    
    device = "cuda:1" if torch.cuda.is_available() else "cpu"
    device = torch.device(device)
    # Load model
    clip_type = {
        'audio': 'LanguageBind_Audio_FT',  # also LanguageBind_Audio
    }
    model = LanguageBind(clip_type=clip_type, cache_dir='.checkpoints')
    model.eval()
    model.to(device)
    
    modality_transform = {c: transform_dict[c](model.modality_config[c]) for c in clip_type.keys()}
    
    audio_model = LanguageBindModel_Super(model)
    print("load languagebind model")
    
    semantic_path = "ckpt/semantic_features/vggsound_languagebind_cls.pt"
    if os.path.exists(semantic_path):
        semantic_feature = torch.load(semantic_path, map_location="cpu")
        semantic_feature = semantic_feature.to(device) 

    audio_model.semantic_modal = semantic_feature
    
    model = TorchWrapper(audio_model.eval(), device)
    
    # data preproccess
    with open(args.eval_list, "r") as f:
        datas = f.read().split('\n')

    label = []
    input_audio = []
    explanations = []

    for data in tqdm(datas[ : args.eval_number]):
        label.append(int(data.strip().split(" ")[-1]))
        input_audio.append(
           read_audio(os.path.join(args.Datasets, data.split(" ")[0]), modality_transform)
        )
        explanations.append(
            np.load(
                os.path.join(args.explanation_method, data.split(" ")[0].replace(".flac", ".npy")))
        )
    
    label_onehot = tf.one_hot(np.array(label), class_number)
    input_audio = np.array(input_audio)
    explanations = np.array(explanations)
    
    # original
    deletion_metric = Deletion(model, input_audio, label_onehot, steps=50, batch_size=32)
    insertion_metric = Insertion(model, input_audio, label_onehot, steps=50, batch_size=32)
    
    deletion_score_org = deletion_metric(explanations)
    insertion_score_org = insertion_metric(explanations)
    
    print("{} Attribution Method Deletion Score: {}".format(args.explanation_method.split("/")[-1], deletion_score_org))
    print("{} Attribution Method Insertion Score: {}".format(args.explanation_method.split("/")[-1], insertion_score_org))
    return 

if __name__ == "__main__":
    args = parse_args()
    main(args)